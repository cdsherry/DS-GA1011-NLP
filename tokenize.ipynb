{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we are going to load the data and get the tokenized data.\n",
    "\n",
    "First we download the .jar file from http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Then we decompress the file and get several folders of txt files with each file contains a comment from IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# load the data from several folders of multiple text files\n",
    "import os\n",
    "def load_data(data_path):\n",
    "    domain = os.path.abspath(data_path)\n",
    "    text = []\n",
    "    file_list = os.listdir(data_path)\n",
    "    for file_path in file_list:\n",
    "        file_name = os.path.join(domain, file_path)\n",
    "        file_content = open(file_name, 'r')\n",
    "        text.append(file_content.read())\n",
    "        file_content.close()\n",
    "    return text\n",
    "\n",
    "# indicate the locations of the folders    \n",
    "train_neg = r'/Users/Sherryzzh/Documents/NLP/HW/aclImdb/train/neg'\n",
    "train_pos = r'/Users/Sherryzzh/Documents/NLP/HW/aclImdb/train/pos'\n",
    "test_neg = r'/Users/Sherryzzh/Documents/NLP/HW/aclImdb/test/neg'\n",
    "test_pos = r'/Users/Sherryzzh/Documents/NLP/HW/aclImdb/test/pos'\n",
    "\n",
    "# read the files in each folder and load them into the list\n",
    "train_neg_data = load_data(train_neg)\n",
    "train_pos_data = load_data(train_pos)\n",
    "test_neg_data = load_data(test_neg)\n",
    "test_pos_data = load_data(test_pos)\n",
    "\n",
    "# choose the first 10000 in negative and positive set respectively, and merge them as the training set\n",
    "train_split = 10000\n",
    "train_data = train_neg_data[:train_split] + train_pos_data[:train_split]\n",
    "# give labels for the training set\n",
    "train_label = [0] * train_split + [1] * train_split\n",
    "\n",
    "# merge the other 2500 negative and 2500 positive data as the validation set\n",
    "val_data = train_neg_data[train_split:] + train_pos_data[train_split:]\n",
    "val_label = [0] * (len(train_neg_data) - train_split) + [1] * (len(train_pos_data) - train_split)\n",
    "\n",
    "# merge the negative and positive test data as the test set\n",
    "test_data = test_neg_data + test_pos_data\n",
    "test_label = [0] * (len(test_neg_data)) + [1] * (len(test_pos_data))\n",
    "\n",
    "# check the length of each dataset\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the tokenization. We fisrt do it for unit words and save the files by pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokenization function \n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import pickle as pkl\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# drop the html tags, lowercase and remove punctuation\n",
    "def tokenize(sent): \n",
    "    dr = re.compile(r'<[^>]+>', re.S)\n",
    "    dd = dr.sub('', sent)\n",
    "    tokens = tokenizer(dd)\n",
    "    uwords = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    return uwords\n",
    "\n",
    "# given a dataset, get the tokens for each data, and get all the tokens for the dataset\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    all_tokens = []    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens_nohtml.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens_nohtml.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens_nohtml.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens_nohtml.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do tokenization for N grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens_nohtml.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens_nohtml.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens_nohtml.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens_nohtml.p\", \"rb\"))\n",
    "\n",
    "# generate N grams from a list of unit words\n",
    "def tokenize_ng(sent, order):\n",
    "    out = []\n",
    "    for oo in range(2, order+1):\n",
    "        ng = [' '.join(t).strip() for t in zip(*[sent[i:] for i in range(oo)])]\n",
    "        out += ng\n",
    "    return sent + out    \n",
    "\n",
    "def tokenize_dataset_ng(data_tokens, order):\n",
    "    token_dataset = []\n",
    "    all_tokens = []    \n",
    "    for sample in data_tokens:\n",
    "        tokens = tokenize_ng(sample, order)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data for 2-gram\")\n",
    "val_data_tokens_2g, _ = tokenize_dataset_ng(val_data_tokens, 2)\n",
    "pkl.dump(val_data_tokens_2g, open(\"val_data_tokens_2g.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data for 2-gram\")\n",
    "test_data_tokens_2g, _ = tokenize_dataset_ng(test_data_tokens, 2)\n",
    "pkl.dump(test_data_tokens_2g, open(\"test_data_tokens_2g.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data for 2-gram\")\n",
    "train_data_tokens_2g, all_train_tokens_2g = tokenize_dataset_ng(train_data_tokens, 2)\n",
    "pkl.dump(train_data_tokens_2g, open(\"train_data_tokens_2g.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_2g, open(\"all_train_tokens_2g.p\", \"wb\"))\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data for 3-gram\")\n",
    "val_data_tokens_3g, _ = tokenize_dataset_ng(val_data_tokens, 3)\n",
    "pkl.dump(val_data_tokens_3g, open(\"val_data_tokens_3g.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data for 3-gram\")\n",
    "test_data_tokens_3g, _ = tokenize_dataset_ng(test_data_tokens, 3)\n",
    "pkl.dump(test_data_tokens_3g, open(\"test_data_tokens_3g.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data for 3-gram\")\n",
    "train_data_tokens_3g, all_train_tokens_3g = tokenize_dataset_ng(train_data_tokens, 3)\n",
    "pkl.dump(train_data_tokens_3g, open(\"train_data_tokens_3g.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_3g, open(\"all_train_tokens_3g.p\", \"wb\"))\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data for 4-gram\")\n",
    "val_data_tokens_4g, _ = tokenize_dataset_ng(val_data_tokens, 4)\n",
    "pkl.dump(val_data_tokens_4g, open(\"val_data_tokens_4g.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data for 4-gram\")\n",
    "test_data_tokens_4g, _ = tokenize_dataset_ng(test_data_tokens, 4)\n",
    "pkl.dump(test_data_tokens_4g, open(\"test_data_tokens_4g.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data for 4-gram\")\n",
    "train_data_tokens_4g, all_train_tokens_4g = tokenize_dataset_ng(train_data_tokens, 4)\n",
    "pkl.dump(train_data_tokens_4g, open(\"train_data_tokens_4g.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_4g, open(\"all_train_tokens_4g.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the effect of Tokenization schemes on the model, we try the tokenization that does not drop the html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the html tags, lowercase and remove punctuation\n",
    "def tokenize(sent): \n",
    "    tokens = tokenizer(sent)\n",
    "    uwords = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    return uwords\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
